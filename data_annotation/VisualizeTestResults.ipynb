{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "186e46f5",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook is used to analyze the testing result and obtain the confidence score threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a70ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm.autonotebook import tqdm\n",
    "import shapely\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "from datetime import datetime as dt\n",
    "\n",
    "\n",
    "IoU_thresh = 0.5\n",
    "r_decision_px = 12\n",
    "\n",
    "# please change these two paths accordingly\n",
    "output_figures_folder_path, detection_results_file_path = (\n",
    "    # Experiment name for the figures\n",
    "    'Faster-RCNN_Tr:Real-LINZ_Test:Real-LINZ',                                                     \n",
    "    # Path to the elaluation results generated by MMDetection or MMYOLO\n",
    "    '../work_dirs/faster-rcnn/LINZ2UGRC/faster-rcnn_train_real_linz_test_real_linz/prediction.pkl'\n",
    ")\n",
    "\n",
    "save_fig = False\n",
    "if save_fig:\n",
    "    os.makedirs(output_figures_folder_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7910a3",
   "metadata": {},
   "source": [
    "## Load Detector Prediction file\n",
    "\n",
    "RUN ONLY ONCE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fb6cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start = dt.now()\n",
    "print(f'Loading started: {t_start}')\n",
    "with open(detection_results_file_path, 'rb') as f:\n",
    "    detection_results = pickle.load(f)\n",
    "t_end = dt.now()\n",
    "print(f'Loading ended:   {t_end} | Duration: {t_end-t_start}')\n",
    "\n",
    "print(f'\\nNum. images: {len(detection_results):,d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05e36c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bboxes_iou(bbox1, bbox2):\n",
    "    if not isinstance(bbox1, shapely.Polygon):\n",
    "        bbox1 = shapely.Polygon(\n",
    "            [\n",
    "                (bbox1[0], bbox1[1]), \n",
    "                (bbox1[2], bbox1[1]), \n",
    "                (bbox1[2], bbox1[3]), \n",
    "                (bbox1[0], bbox1[3])\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    if not isinstance(bbox2, shapely.Polygon):\n",
    "        bbox2 = shapely.Polygon(\n",
    "            [\n",
    "                (bbox2[0], bbox2[1]), \n",
    "                (bbox2[2], bbox2[1]),\n",
    "                (bbox2[2], bbox2[3]),\n",
    "                (bbox2[0], bbox2[3])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    intersection = bbox1.intersection(bbox2)\n",
    "    union = bbox1.union(bbox2)\n",
    "    \n",
    "    return intersection.area / union.area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3045fdb7",
   "metadata": {},
   "source": [
    "## Process Detector Predictions Data\n",
    "\n",
    "RUN ONLY ONCE as it may take some time to process (especially if the dataset is big). After storring the GTs and PREDs data (below) you can directly load them if you want to run this code again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a3475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data_all = []\n",
    "gt_data_all = []\n",
    "for img_res in tqdm(detection_results):\n",
    "    img = Image.open(img_res['img_path'])\n",
    "    \n",
    "    # GTs\n",
    "    gt_instances = []\n",
    "    for label, bbox in zip(img_res['gt_instances']['labels'], img_res['gt_instances']['bboxes']):\n",
    "        bbox_shp = shapely.Polygon([(bbox[0], bbox[1]), (bbox[2], bbox[1]), (bbox[2], bbox[3]), (bbox[0], bbox[3])])\n",
    "        \n",
    "        gt_instances.append(\n",
    "            dict(\n",
    "                label = int(label),\n",
    "                bbox = np.array(bbox),\n",
    "                bbox_shp = bbox_shp,\n",
    "                img_path = img_res['img_path']\n",
    "            )\n",
    "        )\n",
    "\n",
    "    gt_instances = pd.DataFrame(gt_instances)\n",
    "    gt_data_all.append(gt_instances)\n",
    "    \n",
    "    # Re-format predictions\n",
    "    pred_instances = []\n",
    "    for label, bbox, score in zip(img_res['pred_instances']['labels'], img_res['pred_instances']['bboxes'], img_res['pred_instances']['scores']):\n",
    "        # print(int(label), np.array(bbox), float(score))\n",
    "        bbox_shp = shapely.Polygon([(bbox[0], bbox[1]), (bbox[2], bbox[1]), (bbox[2], bbox[3]), (bbox[0], bbox[3])])\n",
    "            \n",
    "        pred_instances.append(\n",
    "            dict(\n",
    "                label = int(label),\n",
    "                bbox = np.array(bbox),\n",
    "                bbox_shp = shapely.Polygon([(bbox[0], bbox[1]), (bbox[2], bbox[1]), (bbox[2], bbox[3]), (bbox[0], bbox[3])]),\n",
    "                score = float(score),\n",
    "                img_path = img_res['img_path'],\n",
    "                is_TP = False,\n",
    "                IoU = None\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if len(pred_instances) == 0:\n",
    "        continue\n",
    "        \n",
    "    pred_instances = pd.DataFrame(pred_instances)\n",
    "    \n",
    "    # Get TPs\n",
    "    IoUs_pred = []\n",
    "    for idx, gt_r in gt_instances.iterrows():\n",
    "        assert gt_r.label==0, 'This version of the code supports only one class: \"small\"'\n",
    "        \n",
    "        IoUs = pred_instances.bbox_shp.apply(lambda x: get_bboxes_iou(x, gt_r.bbox_shp))\n",
    "        IoUs_pred.append(IoUs.to_list())\n",
    "        \n",
    "        mask_IoU = IoUs >= IoU_thresh\n",
    "        mask_TP = pred_instances.is_TP == False\n",
    "        mask = mask_IoU & mask_TP\n",
    "        \n",
    "        if not mask.any():\n",
    "            continue\n",
    "\n",
    "        idx = pred_instances[mask].iloc[0].name\n",
    "        pred_instances.loc[idx, \"is_TP\"] = True\n",
    "        pred_instances.loc[idx, \"IoU\"] = IoUs.loc[idx]\n",
    "\n",
    "    if len(IoUs_pred) > 0:\n",
    "        IoUs_pred = np.array(IoUs_pred)\n",
    "        mask_IoU_missing = ~pred_instances.IoU.notna()\n",
    "        pred_instances.loc[mask_IoU_missing, 'IoU'] = IoUs_pred.max(axis=0)[mask_IoU_missing]\n",
    "    \n",
    "    # pred_data_all.append(pred_instances.drop('bbox_shp', axis=1))\n",
    "    pred_data_all.append(pred_instances)\n",
    "    \n",
    "\n",
    "gt_data_all = pd.concat(gt_data_all)\n",
    "\n",
    "pred_data_all = pd.concat(pred_data_all).sort_values(by='score', ascending=False, ignore_index=True)\n",
    "pred_data_all['is_FP']  = pred_data_all.is_TP.apply(lambda x: not x)\n",
    "pred_data_all['Acc_TP'] = pred_data_all.is_TP.cumsum()\n",
    "pred_data_all['Acc_FP'] = pred_data_all.is_FP.cumsum()\n",
    "pred_data_all['Precision'] = pred_data_all.Acc_TP / (pred_data_all.Acc_TP + pred_data_all.Acc_FP)\n",
    "pred_data_all['Recall'] = pred_data_all.Acc_TP / gt_data_all.shape[0]\n",
    "pred_data_all['F1'] = 2*(pred_data_all.Precision*pred_data_all.Recall) / (pred_data_all.Precision+pred_data_all.Recall)\n",
    "pred_data_all.loc[~pred_data_all.IoU.notna(),'IoU'] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abbfc73",
   "metadata": {},
   "source": [
    "## Store/Load GTs and PREDs data\n",
    "\n",
    "For subsequent runs of this code just load the GTs and PREDs data, saved in advance, by running the cell below and after that [Load](#Load)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044ce28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store gt_data_all and pred_data_all\n",
    "output_folder_data_path = os.path.dirname(detection_results_file_path)\n",
    "\n",
    "pred_data_all_file_path = os.path.join(output_folder_data_path, 'pred_data_all.pkl')\n",
    "gt_data_all_file_path   = os.path.join(output_folder_data_path, 'gt_data_all.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc09552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STORE DATA\n",
    "\n",
    "# Pred\n",
    "t_start = dt.now()\n",
    "print(f'Saving started (pred_data_all): {t_start}')\n",
    "pred_data_all.to_pickle(pred_data_all_file_path)\n",
    "t_end = dt.now()\n",
    "print(f'Saving ended (pred_data_all):   {t_end} | {t_end-t_start}')\n",
    "print()\n",
    "\n",
    "# GT\n",
    "t_start = dt.now()\n",
    "print(f'Saving started (gt_data_all): {t_start}')\n",
    "gt_data_all.to_pickle(gt_data_all_file_path)\n",
    "t_end = dt.now()\n",
    "print(f'Saving ended (gt_data_all):   {t_end} | {t_end-t_start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f59a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "\n",
    "# Pred\n",
    "t_start = dt.now()\n",
    "print(f'Loading started (pred_data_all): {t_start}')\n",
    "pred_data_all = pd.read_pickle(pred_data_all_file_path)\n",
    "t_end = dt.now()\n",
    "print(f'Loading ended (pred_data_all):   {t_end} | {t_end-t_start}')\n",
    "print()\n",
    "\n",
    "# GT\n",
    "t_start = dt.now()\n",
    "print(f'Loading started (gt_data_all): {t_start}')\n",
    "gt_data_all = pd.read_pickle(gt_data_all_file_path)\n",
    "t_end = dt.now()\n",
    "print(f'Loading ended (gt_data_all):   {t_end} | {t_end-t_start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f6eb42",
   "metadata": {},
   "source": [
    "## Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2661a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ade1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2df7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cd4e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcefba5",
   "metadata": {},
   "source": [
    "## Calculate Evaluation Metrics\n",
    "The F1 score is used as the confidence score threshold for labeling test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926862ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = np.array(pred_data_all.Precision.to_list() + [0])\n",
    "recall    = np.array(pred_data_all.Recall.to_list() + [1])\n",
    "\n",
    "n_point_AP_approx = 101\n",
    "\n",
    "recall_approx = np.linspace(0, 1, n_point_AP_approx)\n",
    "\n",
    "precision_approx = []\n",
    "for recall_val in recall_approx:\n",
    "    mask_recall = recall >= recall_val\n",
    "\n",
    "    precision_approx.append(np.max(precision[mask_recall]))\n",
    "\n",
    "precision_approx = np.array(precision_approx)\n",
    "\n",
    "AP = np.sum(precision_approx / n_point_AP_approx)\n",
    "print(f'AP: {AP:.4}  <-- This value should be the same or very similar to the evaluation from MMYOLO or MMDetection.')\n",
    "\n",
    "i_F1_max = pred_data_all.F1.argmax()\n",
    "score_F1_max, F1_max = pred_data_all[['score', 'F1']].iloc[i_F1_max].to_list()\n",
    "print(f'F1_max: {F1_max:.4f} | Score thresh.: {score_F1_max:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1631f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=(10,5))\n",
    "\n",
    "ax = axs[0]\n",
    "ax.plot([0, 1], [1, 0], lw=0.75, c='k', ls='--')\n",
    "pred_data_all.plot(x='Recall', y='Precision', legend=False, label='R-P curve', ylabel='Precision', ax=ax)\n",
    "\n",
    "ax.plot(recall_approx, precision_approx, '--r', label=f'R-P curve ({n_point_AP_approx} pts approx.)')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_title(f'Precision-Recall Curve (AP: {AP:.4})', fontsize=10)\n",
    "ax.axis('square')\n",
    "\n",
    "ax = axs[1]\n",
    "pred_data_all.plot(\n",
    "    x='score', \n",
    "    y=['Recall', 'Precision', 'F1'],\n",
    "    style=['--', '--', '-'],\n",
    "    xlabel='Confidence score',\n",
    "    legend=True, \n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.scatter([score_F1_max], [F1_max], marker='o', facecolor='none', edgecolor='red', zorder=100)\n",
    "\n",
    "ax.annotate(\n",
    "    text=f'Conf. score: {score_F1_max:.4f}\\nF1: {F1_max:.4f}',\n",
    "    xy=(score_F1_max, F1_max),\n",
    "    xytext=(score_F1_max, F1_max*0.5),\n",
    "    arrowprops=dict(\n",
    "        width=0.5, \n",
    "        headwidth=5,\n",
    "        edgecolor='k', \n",
    "        facecolor='k'\n",
    "    ),\n",
    "    c='white',\n",
    "    ha='center',\n",
    "    va='top',\n",
    "    bbox=dict(\n",
    "        edgecolor='none',\n",
    "        facecolor='k',\n",
    "        alpha=0.35\n",
    "    )\n",
    ")\n",
    "ax.axis('square')\n",
    "# ax.legend(loc='upper right')\n",
    "\n",
    "ax.set_title('Precision/Recall/F1 vs. Conf. Score', fontsize=10)\n",
    "\n",
    "fig.suptitle(f'{\" | \".join(output_figures_folder_path.split(\"_\"))}', fontweight='bold')\n",
    "fig.tight_layout()\n",
    "\n",
    "if save_fig:\n",
    "    fig_file_head = os.path.join(output_figures_folder_path, 'P-R curves')\n",
    "    fig.savefig(fig_file_head+'.png', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3fe02d",
   "metadata": {},
   "source": [
    "## [Optional] Visualize prediction statistics\n",
    "Here we provide example code for visualizing positive and negative sample statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b32be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overall predictions distribution\n",
    "fig, ax = plt.subplots(figsize=(6,6.7))\n",
    "\n",
    "d = pred_data_all[pred_data_all.IoU.notna()]\n",
    "d_positive = d[ d.is_TP]\n",
    "d_negative = d[~d.is_TP]\n",
    "\n",
    "scatter_plot_params = dict(\n",
    "    x='score', \n",
    "    y='IoU', \n",
    "    s=6, \n",
    "    alpha=0.2,\n",
    "    edgecolor='none',\n",
    "    ax=ax\n",
    ")\n",
    "d_positive.plot.scatter(**scatter_plot_params, label=f'Positive ({d_positive.shape[0]:,d})', c='g')\n",
    "d_negative.plot.scatter(**scatter_plot_params, label=f'Negative ({d_negative.shape[0]:,d})', c='magenta')\n",
    "\n",
    "ax.axvline(score_F1_max, c='k', ls='--', lw=1)\n",
    "ax.text(\n",
    "    x=score_F1_max+0.01, \n",
    "    y=1-0.01, \n",
    "    s=f'Conf. score thresh. (F1 max):\\n{score_F1_max:.4f}', \n",
    "    ha='left', va='top', rotation='horizontal',\n",
    ")\n",
    "\n",
    "ax.axhline(IoU_thresh, c='k', ls='--', lw=1)\n",
    "ax.text(x=1-0.01, y=IoU_thresh+0.01, s=f'IoU thresh.\\n{IoU_thresh:.3f}', ha='right')\n",
    "\n",
    "ax.set_xlabel('Confidence Score')\n",
    "ax.set_ylabel('Ground Truth IoU')\n",
    "\n",
    "# ax.legend()\n",
    "legend = ax.get_legend()\n",
    "legend.set_title('Prediction Type')\n",
    "# legend._loc_real = 'upper right'\n",
    "for h in legend.legend_handles:\n",
    "    h.set_alpha(1)\n",
    "    h.set_sizes([10])\n",
    "\n",
    "ax.axis('square')\n",
    "ax.axis([-0.05,1.05,-0.05,1.05])\n",
    "\n",
    "ax.set_title('All predictions distribution', fontsize=10)\n",
    "\n",
    "new_line = '\\n'\n",
    "fig.suptitle(f'{new_line.join(output_figures_folder_path.split(\"_\"))}', fontweight='bold')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "if save_fig:\n",
    "    fig_file_head = os.path.join(output_figures_folder_path, 'Predictions Distribution - All')\n",
    "    fig.savefig(fig_file_head+'.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf26af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Positive predictions distribution (TP + FN)\n",
    "fig, ax = plt.subplots(figsize=(6,6.7))\n",
    "\n",
    "d_TP = pred_data_all[pred_data_all.IoU.notna() & pred_data_all.is_TP & (pred_data_all.score >= score_F1_max)]\n",
    "d_FN = pred_data_all[pred_data_all.IoU.notna() & pred_data_all.is_TP & (pred_data_all.score < score_F1_max)]\n",
    "print(\n",
    "    f'Num. TPs: {d_TP.shape[0]:,d}', \n",
    "    f'Num. FNs: {d_FN.shape[0]:,d}', \n",
    "    f'Num. Positive Preds.: {d_TP.shape[0]+d_FN.shape[0]:,d}', \n",
    "    f'Num. GTs: {gt_data_all.shape[0]:,d}',\n",
    "    sep=' | '\n",
    ")\n",
    "\n",
    "# Positives\n",
    "d_TP.plot.scatter(\n",
    "    x='score', \n",
    "    y='IoU', \n",
    "    c='g',\n",
    "    s=6, \n",
    "    alpha=0.2,\n",
    "    edgecolor='none',\n",
    "    label='TP',\n",
    "    ax=ax\n",
    ")\n",
    "ax.text(\n",
    "    x=(1+score_F1_max)/2,\n",
    "    y=(1+IoU_thresh)/2,\n",
    "    s=f'TPs:\\n{d_TP.shape[0]:,d}',\n",
    "    c='white',\n",
    "    ha='center', va='center',\n",
    "    bbox=dict(\n",
    "        edgecolor=None,\n",
    "        facecolor='k',\n",
    "        alpha=0.5\n",
    "    )\n",
    ")\n",
    "\n",
    "d_FN.plot.scatter(\n",
    "    x='score', \n",
    "    y='IoU', \n",
    "    c='lime',\n",
    "    s=6, \n",
    "    alpha=0.2,\n",
    "    edgecolor='none',\n",
    "    label='FN',\n",
    "    ax=ax\n",
    ")\n",
    "ax.text(\n",
    "    x=(score_F1_max)/2,\n",
    "    y=(1+IoU_thresh)/2,\n",
    "    s=f'FNs:\\n{d_FN.shape[0]:,d}',\n",
    "    c='white',\n",
    "    ha='center', va='center',\n",
    "    bbox=dict(\n",
    "        edgecolor=None,\n",
    "        facecolor='k',\n",
    "        alpha=0.5\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "ax.axvline(score_F1_max, c='k', ls='--', lw=1)\n",
    "ax.text(\n",
    "    x=score_F1_max+0.01, \n",
    "    y=1-0.01, \n",
    "    s=f'Conf. score thresh. (F1 max):\\n{score_F1_max:.4f}', \n",
    "    ha='left', va='top', rotation='horizontal',\n",
    ")\n",
    "\n",
    "ax.axhline(IoU_thresh, c='k', ls='--', lw=1)\n",
    "ax.text(x=1-0.01, y=IoU_thresh+0.01, s=f'IoU thresh.\\n{IoU_thresh:.3f}', ha='right')\n",
    "\n",
    "ax.set_xlabel('Confidence Score')\n",
    "ax.set_ylabel('Ground Truth IoU')\n",
    "\n",
    "# ax.legend()\n",
    "legend = ax.get_legend()\n",
    "legend.set_title('Prediction Type')\n",
    "# legend._loc_real = 'upper right'\n",
    "for h in legend.legend_handles:\n",
    "    h.set_alpha(1)\n",
    "    h.set_sizes([10])\n",
    "\n",
    "ax.axis('square')\n",
    "ax.axis([-0.05,1.05,-0.05,1.05])\n",
    "\n",
    "ax.set_title('Positive predictions distribution', fontsize=10)\n",
    "\n",
    "new_line = '\\n'\n",
    "fig.suptitle(f'{new_line.join(output_figures_folder_path.split(\"_\"))}', fontweight='bold')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "if save_fig:\n",
    "    fig_file_head = os.path.join(output_figures_folder_path, 'Predictions Distribution - Positive (FN+TP)')\n",
    "    fig.savefig(fig_file_head+'.png', dpi=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8610ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Negative predictions distribution (TN + FP)\n",
    "d_TN1 = pred_data_all[pred_data_all.IoU.notna()&pred_data_all.is_FP&(pred_data_all.IoU>=IoU_thresh)&(pred_data_all.score<score_F1_max)]\n",
    "d_TN2 = pred_data_all[pred_data_all.IoU.notna()&pred_data_all.is_FP&(pred_data_all.IoU<IoU_thresh)&(pred_data_all.score<score_F1_max)]\n",
    "d_FP1 = pred_data_all[pred_data_all.IoU.notna()&pred_data_all.is_FP&(pred_data_all.IoU>=IoU_thresh)&(pred_data_all.score>=score_F1_max)]\n",
    "d_FP2 = pred_data_all[pred_data_all.IoU.notna()&pred_data_all.is_FP&(pred_data_all.IoU<IoU_thresh)&(pred_data_all.score>=score_F1_max)]\n",
    "# d_FP3 = pred_data_all[~pred_data_all.IoU.notna()&pred_data_all.is_FP&(pred_data_all.score>=score_F1_max)]\n",
    "# print(d_FP3.shape[0])\n",
    "\n",
    "fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(6,6.7))\n",
    "\n",
    "# ax = axs[0]\n",
    "# Negatives\n",
    "scatter_plot_params = dict(\n",
    "    x='score', y='IoU', s=6, \n",
    "    alpha=0.35,\n",
    "    edgecolor='none',\n",
    "    ax=ax\n",
    ")\n",
    "d_TN1.plot.scatter(**scatter_plot_params, label='TN1', c='m', )\n",
    "d_TN2.plot.scatter(**scatter_plot_params, label='TN2', c='orchid')\n",
    "d_FP1.plot.scatter(**scatter_plot_params, label='FP1', c='darkmagenta')\n",
    "d_FP2.plot.scatter(**scatter_plot_params, label='FP2', c='magenta')\n",
    "\n",
    "text_params = dict(\n",
    "    c='white',\n",
    "    ha='center', va='center',\n",
    "    bbox=dict(\n",
    "        edgecolor=None,\n",
    "        facecolor='k',\n",
    "        alpha=0.5\n",
    "    )\n",
    ")\n",
    "ax.text(\n",
    "    x=(score_F1_max)/2,\n",
    "    y=(1+IoU_thresh)/2,\n",
    "    s=f'TN1:\\n{d_TN1.shape[0]:,d}',\n",
    "    **text_params\n",
    ")\n",
    "ax.text(\n",
    "    x=(score_F1_max)/2,\n",
    "    y=(IoU_thresh)/2,\n",
    "    s=f'TN2:\\n{d_TN2.shape[0]:,d}',\n",
    "    **text_params\n",
    ")\n",
    "ax.text(\n",
    "    x=(1+score_F1_max)/2,\n",
    "    y=(1+IoU_thresh)/2,\n",
    "    s=f'FP1:\\n{d_FP1.shape[0]:,d}',\n",
    "    **text_params\n",
    ")\n",
    "ax.text(\n",
    "    x=(1+score_F1_max)/2,\n",
    "    y=(IoU_thresh)/2,\n",
    "    s=f'FP2: {d_FP2.shape[0]:,d}\\n(@IoU0.0: {d_FP2[d_FP2.IoU==0].shape[0]:,d})',\n",
    "    **text_params\n",
    ")\n",
    "\n",
    "ax.axvline(score_F1_max, c='k', ls='--', lw=1)\n",
    "ax.text(\n",
    "    x=score_F1_max+0.01, \n",
    "    y=1-0.01, \n",
    "    s=f'Conf. score\\n(F1 max):\\n{score_F1_max:.4f}', \n",
    "    ha='left', va='top', rotation='horizontal',\n",
    ")\n",
    "\n",
    "ax.axhline(IoU_thresh, c='k', ls='--', lw=1)\n",
    "ax.text(x=1-0.01, y=IoU_thresh+0.01, s=f'IoU thresh.\\n{IoU_thresh:.3f}', ha='right')\n",
    "\n",
    "ax.set_xlabel('Confidence Score')\n",
    "ax.set_ylabel('Ground Truth IoU')\n",
    "\n",
    "if score_F1_max >= 0.5:\n",
    "    legend_loc = 'upper left'\n",
    "else:\n",
    "    legend_loc = 'upper right'\n",
    "\n",
    "legend = ax.legend(ncols=2, loc=legend_loc)\n",
    "# legend = ax.get_legend()\n",
    "legend.set_title('Predictions Type')\n",
    "# legend.set_ncols(2)\n",
    "# # legend._loc_real = 'upper right'\n",
    "for h in legend.legend_handles:\n",
    "    h.set_alpha(1)\n",
    "    h.set_sizes([10])\n",
    "\n",
    "\n",
    "ax.axis('square')\n",
    "ax.axis([-0.05,1.05,-0.05,1.05])\n",
    "\n",
    "ax.set_title('Negative predictions distribution', fontsize=10)\n",
    "\n",
    "new_line = '\\n'\n",
    "fig.suptitle(f'{new_line.join(output_figures_folder_path.split(\"_\"))}', fontweight='bold')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "if save_fig:\n",
    "    fig_file_head = os.path.join(output_figures_folder_path, 'Predictions Distribution - Negative (TN+FP)')\n",
    "    fig.savefig(fig_file_head+'.png', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5a3ff4",
   "metadata": {},
   "source": [
    "## [Optional] Visualize samples\n",
    "Here we provide example code to visualize positive and negative samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae592aa3",
   "metadata": {},
   "source": [
    "### True Positive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6517cf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_score_P = pred_data_all.score >= score_F1_max\n",
    "mask_TP = pred_data_all.is_TP\n",
    "\n",
    "mask_TP = mask_score_P & mask_TP\n",
    "pred_data_TP = pred_data_all[mask_TP]\n",
    "\n",
    "pred_data_TP.shape[0], pred_data_all.shape[0]\n",
    "pred_data_TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89412144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select one TP sample\n",
    "img_path = random.choice(pred_data_TP['img_path'].values)\n",
    "\n",
    "\n",
    "image = Image.open(img_path)\n",
    "\n",
    "gt_data_img   = gt_data_all[gt_data_all.img_path==img_path]\n",
    "pred_data_img = pred_data_all[pred_data_all.img_path==img_path]\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(6,6))\n",
    "\n",
    "ax.imshow(image)\n",
    "\n",
    "gt_color = 'cyan'\n",
    "for i_gt, gt_r in gt_data_img.iterrows():\n",
    "    ax.plot(*gt_r.bbox_shp.boundary.coords.xy, c=gt_color, ls='--', lw=1)\n",
    "    ax.scatter(*gt_r.bbox_shp.centroid.coords.xy, fc=gt_color, marker='x', lw=1)\n",
    "\n",
    "\n",
    "for i_pred, pred_r in pred_data_img.iterrows():\n",
    "    if not pred_r.is_TP:\n",
    "        continue\n",
    "\n",
    "    if pred_r.score >= score_F1_max:\n",
    "        pred_color = 'lime'\n",
    "        \n",
    "    else:\n",
    "        # pred_color = 'magenta'\n",
    "        pred_color = 'red'\n",
    "    \n",
    "    print(i_pred, pred_r.score)\n",
    "    \n",
    "    ax.plot(*pred_r.bbox_shp.boundary.coords.xy, c=pred_color)\n",
    "    ax.scatter(*pred_r.bbox_shp.centroid.coords.xy, fc=pred_color)\n",
    "\n",
    "    annot_params = dict(\n",
    "        x=(pred_r.bbox[0]+pred_r.bbox[2])/2, \n",
    "        s=f'Conf:{pred_r.score:.2f}\\nIoU:{pred_r.IoU:.2f}', \n",
    "        c=pred_color, \n",
    "        backgroundcolor=(0.,0.,0.,0.25),\n",
    "        bbox=dict(edgecolor='none', facecolor='k', alpha=0.3),\n",
    "        ha='center',\n",
    "        fontsize=8\n",
    "    )\n",
    "    if pred_r.bbox_shp.centroid.coords.xy[1][0] / image.size[1] >= 0.5:\n",
    "        annot_params.update(\n",
    "            dict(\n",
    "                y=pred_r.bbox[1]-1,\n",
    "                va='bottom'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        annot_params.update(\n",
    "            dict(\n",
    "                y=pred_r.bbox[3]+1,\n",
    "                va='top'\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    ax.text(**annot_params)\n",
    "\n",
    "# Draw edge samples grid\n",
    "line_params = dict(lw=0.75, ls='dashed', c='k')\n",
    "ax.axvline(r_decision_px, **line_params)\n",
    "ax.axvline(image.size[0]-r_decision_px-1, **line_params)\n",
    "ax.axhline(r_decision_px, **line_params)\n",
    "ax.axhline(image.size[1]-r_decision_px-1, **line_params)\n",
    "\n",
    "\n",
    "ax.set_title(f'True Positive:{os.path.basename(img_path)}')\n",
    "ax.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230aad23",
   "metadata": {},
   "source": [
    "### False Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9402a471",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_score_N = pred_data_all.score < score_F1_max\n",
    "mask_TP = pred_data_all.is_TP\n",
    "\n",
    "mask_FN = mask_score_N & mask_TP\n",
    "pred_data_FN = pred_data_all[mask_FN]\n",
    "pred_data_FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4aae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select one FN sample\n",
    "img_path = random.choice(pred_data_FN['img_path'].values)\n",
    "image = Image.open(img_path)\n",
    "\n",
    "\n",
    "gt_data_img   = gt_data_all[gt_data_all.img_path==img_path]\n",
    "pred_data_img = pred_data_all[pred_data_all.img_path==img_path]\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(6,6))\n",
    "\n",
    "ax.imshow(image)\n",
    "\n",
    "for i_gt, gt_r in gt_data_img.iterrows():\n",
    "    ax.plot(*gt_r.bbox_shp.boundary.coords.xy, c='lime', ls='--', lw=1)\n",
    "    ax.scatter(*gt_r.bbox_shp.centroid.coords.xy, fc='lime', marker='x', lw=1)\n",
    "\n",
    "\n",
    "for i_pred, pred_r in pred_data_img.iterrows():\n",
    "    if not pred_r.is_TP:\n",
    "        continue\n",
    "\n",
    "    if pred_r.score >= score_F1_max:\n",
    "        pred_color = 'lime'\n",
    "    else:\n",
    "        pred_color = 'red'\n",
    "    \n",
    "    print(i_pred, pred_r.score)\n",
    "    \n",
    "    ax.plot(*pred_r.bbox_shp.boundary.coords.xy, c=pred_color)\n",
    "    ax.scatter(*pred_r.bbox_shp.centroid.coords.xy, fc=pred_color)\n",
    "    # ax.text(pred_r.bbox[0], pred_r.bbox[1]-2, f'Conf:{pred_r.score:.4f}', c='red')\n",
    "    annot_params = dict(\n",
    "        x=(pred_r.bbox[0]+pred_r.bbox[2])/2,\n",
    "        s=f'Conf:{pred_r.score:.3f}\\nIoU:{pred_r.IoU:.3f}',\n",
    "        c=pred_color, \n",
    "        # backgroundcolor=(0.,0.,0.,0.25),\n",
    "        bbox=dict(edgecolor='none', facecolor='k', alpha=0.35),\n",
    "        ha='center',\n",
    "        fontsize=8\n",
    "    )\n",
    "    if pred_r.bbox_shp.centroid.coords.xy[1][0] / image.size[1] >= 0.5:\n",
    "        annot_params.update(\n",
    "            dict(\n",
    "                y=pred_r.bbox[1]-1,\n",
    "                va='bottom'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        annot_params.update(\n",
    "            dict(\n",
    "                y=pred_r.bbox[3]+1,\n",
    "                va='top'\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    ax.text(**annot_params)\n",
    "\n",
    "# Draw edge samples grid\n",
    "line_params = dict(lw=0.75, ls='dashed', c='k')\n",
    "ax.axvline(r_decision_px, **line_params)\n",
    "ax.axvline(image.size[0]-r_decision_px-1, **line_params)\n",
    "ax.axhline(r_decision_px, **line_params)\n",
    "ax.axhline(image.size[1]-r_decision_px-1, **line_params)\n",
    "\n",
    "ax.set_title(f'False Negative:{os.path.basename(img_path)}')\n",
    "ax.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e66b954",
   "metadata": {},
   "source": [
    "### False Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a5099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_score_P = pred_data_all.score >= score_F1_max\n",
    "mask_IoU1    = pred_data_all.IoU.notna()\n",
    "mask_FP      = pred_data_all.is_FP\n",
    "\n",
    "mask_FP1 = mask_score_P & mask_FP & mask_IoU1\n",
    "pred_data_FP1 = pred_data_all[mask_FP1]\n",
    "pred_data_FP1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ec3304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select one FP sample\n",
    "img_path = random.choice(pred_data_FP1['img_path'].values)\n",
    "\n",
    "image = Image.open(img_path)\n",
    "\n",
    "gt_data_img   = gt_data_all[gt_data_all.img_path==img_path]\n",
    "pred_data_img = pred_data_all[pred_data_all.img_path==img_path]\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(6,6))\n",
    "\n",
    "ax.imshow(image)\n",
    "\n",
    "for i_gt, gt_r in gt_data_img.iterrows():\n",
    "    ax.plot(*gt_r.bbox_shp.boundary.coords.xy, c='lime', ls='--', lw=1)\n",
    "    ax.scatter(*gt_r.bbox_shp.centroid.coords.xy, fc='lime', marker='x', lw=1)\n",
    "\n",
    "\n",
    "for i_pred, pred_r in pred_data_img.iterrows():\n",
    "    # if not pred_r.is_TP:\n",
    "    #     continue\n",
    "    \n",
    "    if pred_r.score < score_F1_max:\n",
    "        continue\n",
    "\n",
    "    if pred_r.is_TP:\n",
    "        # continue\n",
    "        pred_color = 'lime'\n",
    "        label = 'TP'\n",
    "        \n",
    "    else:\n",
    "        pred_color = 'red'\n",
    "        label = 'FP'\n",
    "    \n",
    "    print(i_pred, pred_r.score)\n",
    "    \n",
    "    ax.plot(*pred_r.bbox_shp.boundary.coords.xy, c=pred_color)\n",
    "    ax.scatter(*pred_r.bbox_shp.centroid.coords.xy, fc=pred_color)\n",
    "    \n",
    "    # ax.text(pred_r.bbox[0], pred_r.bbox[1]-2, f'Conf:{pred_r.score:.4f}', c='red')\n",
    "    annot_params = dict(\n",
    "        # x=pred_r.bbox[0], \n",
    "        x=pred_r.bbox_shp.centroid.x,\n",
    "        s=f'[{label}] Conf:{pred_r.score:.4f}',\n",
    "        c=pred_color,\n",
    "        ha='center',\n",
    "        backgroundcolor=(0.,0.,0.,0.25),\n",
    "        fontsize=8\n",
    "    )\n",
    "    if pred_r.bbox_shp.centroid.coords.xy[1][0] / image.size[1] >= 0.5:\n",
    "        annot_params.update(\n",
    "            dict(\n",
    "                y=pred_r.bbox[1]-1,\n",
    "                va='bottom'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        annot_params.update(\n",
    "            dict(\n",
    "                y=pred_r.bbox[3]+1,\n",
    "                va='top'\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    ax.text(**annot_params)\n",
    "\n",
    "\n",
    "# Draw edge samples grid\n",
    "line_params = dict(lw=0.75, ls='dashed', c='k')\n",
    "ax.axvline(r_decision_px, **line_params)\n",
    "ax.axvline(image.size[0]-r_decision_px-1, **line_params)\n",
    "ax.axhline(r_decision_px, **line_params)\n",
    "ax.axhline(image.size[1]-r_decision_px-1, **line_params)\n",
    "\n",
    "\n",
    "ax.set_title(f'False Positive:{os.path.basename(img_path)}')\n",
    "ax.grid(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
